[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BarmyBlog",
    "section": "",
    "text": "Why masked arrays are useful for data science. Part 1\n\n\n\n\n\n\n\nnumpy\n\n\nmissing data\n\n\npairwise comparisons\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nCarl Gaspar\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/masked-arrays/index.html",
    "href": "posts/masked-arrays/index.html",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "",
    "text": "Ever needed to compare lots of variables and struggled with missing data?\nA surprisingly wide range of measures that are core to data science can be accelerated with the use of matrix multiplication. You just need to get creative when reformulating your computation.\nBut real-world data has missing values and these can make matrix multiplication useless. Does that mean one must resort to inefficient for-loops to keep track of missing values?\nNo.Â Using Numpy masked arrays one can still reap the benefits of matrix multiplication whilst making full use of oneâ€™s data.\nIn this post I cover:\nThe covariance matrix is another measure we can efficiently compute using MM and masked arrays. I am leaving that for another post where I also describe existing solutions (coming soon).\nThe solution I describe here for efficient computation of percentage agreement (and Cohenâ€™s kappa coefficient) is not something in any major Python package.\nBut before we get to that, letâ€™s talk about matrix multiplication (MM)."
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-matmul",
    "href": "posts/masked-arrays/index.html#sec-matmul",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "1 Sums of products. Lots of them, fast",
    "text": "1 Sums of products. Lots of them, fast\nAs implemented in most popular packages like Pythonâ€™s Numpy, MM letâ€™s you rapidly calculate sums-of-products for many pairs of variables. So letâ€™s start with sums-of-products.\nHere is a single variable and itâ€™s sum-of-products:\n\nimport numpy as np\nx = np.array([1, 2, 3])\nsum_of_products = (x**2).sum()\nprint(sum_of_products)\n\n14\n\n\nNow we consider another variable y and calculate the sum-of-products between x and y:\n\ny = np.array([3, 2, 1])\nproducts = np.multiply(x, y)\nsum_of_products = products.sum()\nprint(products)\nprint(sum_of_products)\n\n[3 4 3]\n10\n\n\nNow we package our x and y as the column vectors of a single matrix:\n\nX = np.array([[1, 3],\n              [2, 2],\n              [3, 1]])\nnumber_observations, number_variables = X.shape\nprint(f\"{number_observations} observations (rows)\")\nprint(f\"{number_variables} variables (columns)\")\n\n3 observations (rows)\n2 variables (columns)\n\n\nWhy did with do this? If you matrix multiply X with itself you get something interesting:\n\nS = np.dot(X.transpose(), X)\nprint(S)\n\n[[14 10]\n [10 14]]\n\n\nS[0,0] is the sum-of-products of x with itself. S[0,1] is the sum-of-products of x with y. S[1,0] is also the sum-of-products of x with y. And S[1,1] is the sum-of-products of y with itself.\nIn other words, matrix multiplication (MM) gives you the sum-of-products for every pairwise comparison.\nAnd as mentioned earlier, MM is computed very quickly using packages like Numpy.\nBut why do we care about sums-of-products for every pairwise comparison?\nSo many interesting measures/statistics can be reformulated as an efficient sequence of MM.\nSo with just a little bit of cleverness you can turn your very inefficient for-loops into a very efficient sequence of matrix operations."
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-agreement",
    "href": "posts/masked-arrays/index.html#sec-agreement",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "2 Percentage agreement the slow way",
    "text": "2 Percentage agreement the slow way\nImagine that 8 people filled out a survey consisting of 4 yes/no items:\n\nimport pandas as pd\nX = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n              [0, 0, 0, 0, 1, 1, 1, 1],\n              [0, 1, 0, 1, 0, 1, 0, 1],\n              [1, 1, 1, 1, 0, 0, 0, 0]]).transpose()\n\ncol_labels = ['Item ' + str(i) for i in range(1,5)]\nrow_labels = ['Person ' + str(i) for i in range(1,9)]\nprint(pd.DataFrame(X, columns=col_labels, index=row_labels))\n\n          Item 1  Item 2  Item 3  Item 4\nPerson 1       0       0       0       1\nPerson 2       0       0       1       1\nPerson 3       0       0       0       1\nPerson 4       0       0       1       1\nPerson 5       1       1       0       0\nPerson 6       1       1       1       0\nPerson 7       1       1       0       0\nPerson 8       1       1       1       0\n\n\nYou want to know if there are associations among these items.\nA glance at this stylized data set suffices.\nItem 1 is perfectly positively associated with item 2 in this sample; 100 percent of the responses are in agreement.\nItem 1 is has no association with item 3 in this sample; guessing someoneâ€™s response to item 3 based their response to 1 is no better than a coin flip (50 percent).\nItem 1 is perfectly negatively associated with item 4; 0 percent of the responses are in agreement.\nIf you have a much larger data set, you might decide to use for-loops like this:\nNOTE TO SELF\n\n#def pa_loop(X):         # ðŸš€\n#def pa_loop_t(X):\n#def pa_vect(X):         # ðŸš€\n#def pa_loop_missing(X): # ðŸš€\n#def pa_vect_missing(X): # ðŸš€\n\n\ndef pa_loop(X): # ðŸš€\n    number_samples, number_variables = X.shape\n    percent_agreement = np.zeros((number_variables, number_variables))\n    for item_a in range(number_variables):\n        for item_b in range(item_a+1, number_variables):\n            percent_agreement[item_a, item_b] = (X[:, item_a]==X[:, item_b]).sum()\n    percent_agreement /= number_samples\n    return(percent_agreement)\n\nBut this can be slow for large data sets:\n\nfrom time import perf_counter\nnumber_samples, number_variables = 1000, 1000\nX = np.random.choice([0., 1.], size=(number_samples, number_variables), replace=True)\ntic = perf_counter()\npercent_agreement = pa_loop(X)\ntoc = perf_counter()\nprint(f\"Computing all pairwise percentage agreements took {toc-tic:0.4f} seconds.\")\n\nComputing all pairwise percentage agreements took 3.2916 seconds.\n\n\nThis may not seem so bad. But consider that\n\nThings will be much worse when missing data are considered (SectionÂ 4).\nData exploration might mean iterating this code many times.\nPercentage agreement is simpler than some other measures/statistics.\n\nHow can we use matrix multiplication to speed things up? Weâ€™ll have to use some tricks but itâ€™s not that hard."
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-vectorization",
    "href": "posts/masked-arrays/index.html#sec-vectorization",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "3 Percentage agreement the fast way",
    "text": "3 Percentage agreement the fast way\nHere is a simple case with yes and no responses for 2 items coded as 1s and 0s respectively:\n\n\n\nitem 1\nitem 2\nagreement\nyes-yes\nno-no\n\n\n\n\n0\n0\n1\n0\n1\n\n\n0\n1\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n0\n\n\n\nWhat we want is a column like agreement above whose sum (2) gives us the number of agreements. We then divide 2 by 4 to get 50-percent agreement.\nTreating item 1 as a row vector and item 2 as a column vector, we can perform matrix multiplication to get the sum of yes-yes (1). As an intermediate step in matrix multiplication, our 2 vectors are multiplied value-by-value giving us the yes-yes column above. Summing is the final step of matrix multiplication (1).\nAgreements can be either yes-yes or no-no so we still need to obtain that sum before we can measure number of agreements. Thatâ€™s easy. We simply flip the values in item 1 and item 2 from 0 to 1, and 1 to 0:\n\\[\nnew = \\lvert old-1 \\rvert\n\\]\nMatrix multiplication of these complementary vectors for item 1 and item 2 (not shown here) gives us our sum of no-no, whose intermediate step is the no-no column above.\nThe above example is for 2 items but the power of matrix multiplication is that we are multiplying matrices â€“ as many items as we want. And the result is a matrix of all pairwise comparisons. In other words, we can obtain a matrix of percentage agreement for all pairwise comparisons using a simple sequence of matrix-based operations, like this:\n\ndef pa_vect(X):\n    number_samples, number_variables = X.shape # (n x k)\n    yesYes = np.dot(X.transpose(), X)          # counts of yes-yes (k x k)\n    np.abs(X-1, out=X)                         # [0,1] -> [1,0]\n    noNo = np.dot(X.transpose(), X)            # counts of no-no   (k x k)\n    S = yesYes + noNo                          # counts of agreements (k x k)\n    A = S / number_samples                     # percentage agreements (k x k)   \n    return(A)\n\nHow much faster is this compared to the loop-based computation?\n\nnumber_samples = 1000\nnumber_variables = [10, 50, 100, 500, 1000]\nseconds = np.zeros((len(number_variables), 2))\nfor i, nvl in enumerate(number_variables):\n    X = np.random.choice([0., 1.], size=(number_samples, nvl), replace=True)\n    tic = perf_counter(); percent_agreement = pa_loop(X); toc = perf_counter()\n    seconds_loop = toc-tic\n    tic = perf_counter(); percent_agreement = pa_vect(X); toc = perf_counter()\n    seconds_vect = toc-tic\n    seconds[i, 0] = seconds_loop\n    seconds[i, 1] = seconds_vect\n    \ndf = pd.DataFrame(seconds, \n                  columns=['loop','matrix-based'],\n                  index=number_variables)\ndf\n\n\n\n\n\n  \n    \n      \n      loop\n      matrix-based\n    \n  \n  \n    \n      10\n      0.000851\n      0.003188\n    \n    \n      50\n      0.012009\n      0.005438\n    \n    \n      100\n      0.057433\n      0.003524\n    \n    \n      500\n      0.845797\n      0.057247\n    \n    \n      1000\n      3.122708\n      0.060030\n    \n  \n\n\n\n\nLetâ€™s examine the speed up graphically:\n\nimport matplotlib.pyplot as plt\n\ndf.plot(loglog=True, xlabel=\"Number of variables\", ylabel=\"Seconds\")\nplt.show()\n\n\n\n\nMatrix-based computation is definitely faster!\nHow much faster exactly?\nDividing the time taken by loop-based computation by the time taken by matrix-based computation gives us a speed-up. The larger the number the stronger the advantage for matrix-based computation:\n\nspeed_up = seconds[:,0] / seconds[:,1]\nplt.semilogx(number_variables, speed_up)\nplt.xlabel(\"Number of variables\")\nplt.ylabel(\"Speed advantage\")\nplt.title(\"Matrix-based operation beats loops by miles\")\nplt.show()"
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-curse",
    "href": "posts/masked-arrays/index.html#sec-curse",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "4 The curse of missing data",
    "text": "4 The curse of missing data"
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-masked-arrays",
    "href": "posts/masked-arrays/index.html#sec-masked-arrays",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "5 Masked arrays to the rescue",
    "text": "5 Masked arrays to the rescue"
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-details",
    "href": "posts/masked-arrays/index.html#sec-details",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "6 Nasty details for those interested",
    "text": "6 Nasty details for those interested"
  },
  {
    "objectID": "posts/masked-arrays/index.html#sec-takeaway",
    "href": "posts/masked-arrays/index.html#sec-takeaway",
    "title": "Why masked arrays are useful for data science. Part 1",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nIf you need to compute a measure/statistic for all pairwise comparisons among a large set of variables, then you can probably speed things up a lot with matrix multiplications. It may take some creativity, but this hopefully this blog post can provide some inspiration.\nIf you also have missing data, then you can use matrix multiplications to obtain even more gains in efficiency! Simply use Numpy masked arrays.\nIf you do decide to try out a similar solution for your measure/statistic, pay heed to SectionÂ 6. And watch out for Jax. While it currently does not support masked arrays, perhaps it or some external package using Jax will do so in the future. That could be very useful.\nThe solution I present here for percentage agreement and Cohenâ€™s kappa is not currently implemented in any major Python package. I do not think that my solution would be useful for scikit-learn, as Cohenâ€™s kappa is primarily used here for model comparison with ground-truth labels. Unless you wanted to obtain a detailed picture of categorization errors among hundreds of thousands of models â€¦ However, it might have a place in some other package.\nFinally, watch out for my next post on how missing data are dealt with by current solutions that measure the covariance matrix, and why that matters."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable R code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]