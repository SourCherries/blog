---
title: "Why masked arrays are useful for data science"
author: "Carl Gaspar"
date: "2023-08-01"
categories: [numpy, missing data, pairwise comparisons]
image: "image.jpg"
---

Ever wanted to know why Numpy masked arrays are useful? Ever needed to compare lots of variables and struggled with missing data?

Computations that are core to data science can be accelerated with the use of matrix operations. But real-world data has missing values and these can make matrix computations useless. Does that mean one must resort to inefficient for-loops when missing data is present?

No. Using Numpy masked arrays one can still reap the benefits of matrix operations whilst making full use of one's data.

What kinds of things are we measuring here? We focus on pairwise comparisons among a large set of variables. The most familiar example is the **covariance matrix** but that is not all. 

The approach described here generalizes to many cases where pairwise comparisons of some kind are to be calculated. I describe how to use masked arrays to calculate **percentage agreement** (and **Cohen's kappa coefficient**). That is what I happened to be working on recently.

So we'll start with covariance matrices. Then I'll show you a trick to quickly get matrices of percentage agreement (and **Cohen's kappa coefficient**) even with missing data.

## Covariance matrices

I will go over a simple example that will remind you what a covariance matrix is, and how it can be calculated using matrix multiplication.

Here is some example data with 3 samples of 3 variables:

```{python}
import numpy as np
import pandas as pd

X = np.array([[-3, 3, 0],
              [ 0, 0, 0],
              [ 3,-3, 0]])

number_samples, number_variables = X.shape
```

We will get the covariance matrix using both Numpy and Pandas because there are some important differences. So here's the data in Pandas:

```{python}
X_ = pd.DataFrame(X, columns = ["A", "B", "C"], index = ["sample " + str(i) for i in range(1,4)])              
print(X_)
```

The covariance matrix is a matrix showing the covariance between each pair of variables.

```{python}
# Using Numpy
#   rowvar is False so that columns are variables
#   bias is True to simplify our examples
C = np.cov(X, rowvar=False, bias=True)

# Using Pandas
#   ddof is 0 (same as bias is True above)
C_ = X_.cov(ddof=0)
print(C_)
```

As you can see the covariance between variables `A` and `B` is $-6$.
The covariance between `C` and both `A` and `B` is $0$.

Covariance is the expected product between 2 variables that are first centered at 0. We estimate it with the mean product. 

Our variables are already centered at zero:

```{python}
C_.sum(axis=0)
```

So we just need to:

1. measure the sum of products between each pair of variables
2. divide those products by 3 (the sample size)

Step (1) can be done with matrix multiplication like this:

```{python}
products = np.matmul(X.transpose(), X)

C_another_way = products / number_samples

assert (C_another_way==C).all()
```

Step one (matrix multiplication) works really fast in Numpy. We used it here to get the covariance matrix but I'll show you how to also get a matrix of *percentage agreement*.

Before we get to that let's just make sure we know what's going on first.

$C[0,1]$ is the covariance between the variables `A` and `B`.

$products[0,1]$ is the sum of products between `A` and `B`.

So $products[0,1] = (-3\times3) + (0\times0) + (3\times-3) = -18$

$products[1,2]$ is the sum of products between `B` and `C`.

And $products[0,1] = (3\times0) + (0\times0) + (-3\times0) = 0$



## Covariance matrices with missing data


## Solution using masked arrays

## Generalizing this approach to measure percentage agreement


### End