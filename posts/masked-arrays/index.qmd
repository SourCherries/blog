---
title: "Why masked arrays are useful for data science"
author: "Carl Gaspar"
date: "2023-08-01"
categories: [numpy, missing data, pairwise comparisons]
image: "image.jpg"
---

Ever needed to compare lots of variables and struggled with missing data?

A surprisingly wide range of measures that are core to data science can be accelerated with the use of matrix multiplication. You just need to get creative when reformulating your computation.

But real-world data has missing values and these can make matrix multiplication useless. Does that mean one must resort to inefficient for-loops to keep track of missing values?

No. Using Numpy masked arrays one can still reap the benefits of matrix multiplication whilst making full use of one's data.

In this post I cover:

1.  What matrix multiplication (**MM**) is.
2.  How we can use **MM** to efficiently measure **percentage agreement**.
3.  How we can then measure **Cohen's kappa coefficient**.
4.  Why missing data are a problem for **MM**.
5.  Show that Numpy masked arrays solves this problem.

The **covariance matrix** is another measure we can efficiently compute using **MM** and masked arrays. I am leaving that for another post where I also describe existing solutions.

In constrast, the solution I describe here for efficient computation of **percentage agreement** (**Cohen's kappa coefficient**) is not something in any major Python package.

But before we get to that, let's talk about **matrix multiplication** (\*\*MM\*).

## Sums of products. Lots of them, fast

As implemented in most popular packages like Python's Numpy, **MM** let's you rapidly calculate sums-of-products for many pairs of variables. So let's start with sums-of-products.

Here is a single variable and it's sum-of-products:

```{python}
import numpy as np
x = np.array([1, 2, 3])
sum_of_products = (x**2).sum()
print(sum_of_products)
```
Now we consider another variable `y` and calculate the sum-of-products between `x` and `y`:

```{python}
y = np.array([3, 2, 1])
products = np.multiply(x, y)
sum_of_products = products.sum()
print(products)
print(sum_of_products)
```
Now we package our `x` and `y` as the column vectors of a single matrix:
```{python}
X = np.array([[1, 3],
              [2, 2],
              [3, 1]])
number_observations, number_variables = X.shape
print(f"{number_observations} observations (rows)")
print(f"{number_variables} variables (columns)")
```
Why did with do this? If you matrix multiply `X` with itself you get something interesting:

```{python}
S = np.dot(X.transpose(), X)
print(S)
```
`S[0,0]` is the sum-of-products of `x` with itself. `S[0,1]` is the sum-of-products of `x` with `y`. `S[1,0]` is also the sum-of-products of `x` with `y`. And `S[1,1]` is the sum-of-products of `y` with itself.

In other words, matrix multiplication (**MM**) gives you the sum-of-products for every pairwise comparison.

And as mentioned earlier, **MM** is computed very quickly using packages like Numpy.

But why do we care about sums-of-products for every pairwise comparison? 

So many interesting measures/statistics can be reformulated as an efficient sequence of **MM**. 

So with just a little bit of cleverness you can turn your very inefficient for-loops into a very efficient sequence of matrix operations.

## Efficient computation of percentage agreement


## The curse of missing data

## Masked arrays to the rescue



### End
