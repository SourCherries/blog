{
  "hash": "31f35128324ad12aa3929387f24b08fd",
  "result": {
    "markdown": "---\ntitle: \"Why masked arrays are useful for data science. Part 1\"\nauthor: \"Carl Gaspar\"\ndate: \"2023-08-01\"\ncategories: [numpy, missing data, pairwise comparisons]\nimage: \"image.jpg\"\nnumber-sections: true\n---\n\nEver needed to compare lots of variables and missing data made things super slow?\n\nA surprisingly wide range of measures that are core to data science can be accelerated with the use of matrix multiplication. You just need to get creative when reformulating your computation.\n\nBut real-world data has missing values and these can make matrix multiplication useless. Does that mean one must resort to inefficient for-loops to keep track of missing values?\n\nNo. Using Numpy masked arrays one can still reap the benefits of matrix multiplication whilst making full use of one's data.\n\nHere I demonstrate the use of numpy masked arrays for the efficient calculation of **percentage agreement** among many binary variables.\n\nIn this post I cover:\n\n1.  What matrix multiplication (**MM**) is (@sec-matmul)\n2.  Percentage agreement the slow way (@sec-agreement)\n2.  Percentage agreement (and Cohen's kappa) the fast way (@sec-vectorization)\n3.  Why missing data are a problem for **MM** (@sec-curse)\n4.  Show that Numpy masked arrays solves this problem (@sec-masked-arrays)\n5.  Give some pointers on optimization (@sec-details)\n6.  The takeaway. Go to @sec-takeaway if you're impatient!\n\nThe **covariance matrix** is another measure we can efficiently compute using **MM** and masked arrays. I am leaving that for another post where I also describe existing solutions (**coming soon**).\n\nThe solution I describe here for efficient computation of **percentage agreement** (and **Cohen's kappa coefficient**) is not something in any major Python package.\n\nBut before we get to that, let's talk about **matrix multiplication** (**MM**).\n\n## Sums of products. Lots of them, fast {#sec-matmul}\n\nAs implemented in most popular packages like Python's Numpy, **MM** let's you rapidly calculate sums-of-products for many pairs of variables. So let's start with sums-of-products.\n\nHere is a single variable and it's sum-of-products:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nx = np.array([1, 2, 3])\nsum_of_products = (x**2).sum()\nprint(sum_of_products)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n14\n```\n:::\n:::\n\n\nNow we consider another variable `y` and calculate the sum-of-products between `x` and `y`:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ny = np.array([3, 2, 1])\nproducts = np.multiply(x, y)\nsum_of_products = products.sum()\nprint(products)\nprint(sum_of_products)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[3 4 3]\n10\n```\n:::\n:::\n\n\nNow we package our `x` and `y` as the column vectors of a single matrix:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nX = np.array([[1, 3],\n              [2, 2],\n              [3, 1]])\nnumber_observations, number_variables = X.shape\nprint(f\"{number_observations} observations (rows)\")\nprint(f\"{number_variables} variables (columns)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3 observations (rows)\n2 variables (columns)\n```\n:::\n:::\n\n\nWhy did with do this? If you matrix multiply `X` with itself you get something interesting:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nS = np.dot(X.transpose(), X)\nprint(S)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[14 10]\n [10 14]]\n```\n:::\n:::\n\n\n`S[0,0]` is the sum-of-products of `x` with itself. `S[0,1]` is the sum-of-products of `x` with `y`. `S[1,0]` is also the sum-of-products of `x` with `y`. And `S[1,1]` is the sum-of-products of `y` with itself.\n\nIn other words, matrix multiplication (**MM**) gives you the sum-of-products for every pairwise comparison.\n\nAnd as mentioned earlier, **MM** is computed very quickly using packages like Numpy.\n\nBut why do we care about sums-of-products for every pairwise comparison?\n\nSo many interesting measures/statistics can be reformulated as an efficient sequence of **MM**.\n\nSo with just a little bit of cleverness you can turn your very inefficient for-loops into a very efficient sequence of matrix operations.\n\n## Percentage agreement the slow way {#sec-agreement}\n\nImagine that 8 people filled out a survey consisting of 4 yes/no items:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nX = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n              [0, 0, 0, 0, 1, 1, 1, 1],\n              [0, 1, 0, 1, 0, 1, 0, 1],\n              [1, 1, 1, 1, 0, 0, 0, 0]]).transpose()\n\ncol_labels = ['Item ' + str(i) for i in range(1,5)]\nrow_labels = ['Person ' + str(i) for i in range(1,9)]\nprint(pd.DataFrame(X, columns=col_labels, index=row_labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Item 1  Item 2  Item 3  Item 4\nPerson 1       0       0       0       1\nPerson 2       0       0       1       1\nPerson 3       0       0       0       1\nPerson 4       0       0       1       1\nPerson 5       1       1       0       0\nPerson 6       1       1       1       0\nPerson 7       1       1       0       0\nPerson 8       1       1       1       0\n```\n:::\n:::\n\n\nYou want to know if there are associations among these items.\n\nA glance at this stylized data set suffices.\n\nItem 1 is perfectly positively associated with item 2 in this sample; 100 percent of the responses are in agreement.\n\nItem 1 is has no association with item 3 in this sample; guessing someone's response to item 3 based their response to 1 is no better than a coin flip (50 percent).\n\nItem 1 is perfectly negatively associated with item 4; 0 percent of the responses are in agreement.\n\nIf you have a much larger data set, you might decide to use for-loops like this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef pa_loop(X):\n    number_samples, number_variables = X.shape\n    percent_agreement = np.zeros((number_variables, number_variables))\n    for item_a in range(number_variables):\n        for item_b in range(item_a+1, number_variables):\n            percent_agreement[item_a, item_b] = (X[:, item_a]==X[:, item_b]).sum()\n    percent_agreement /= number_samples\n    return(percent_agreement)\n```\n:::\n\n\nBut this can be slow for large data sets:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom time import perf_counter\nnumber_samples, number_variables = 1000, 1000\nX = np.random.choice([0., 1.], size=(number_samples, number_variables), replace=True)\ntic = perf_counter()\npercent_agreement = pa_loop(X)\ntoc = perf_counter()\nprint(f\"Computing all pairwise percentage agreements took {toc-tic:0.4f} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComputing all pairwise percentage agreements took 4.4096 seconds.\n```\n:::\n:::\n\n\nThis may not seem so bad. But consider that\n\n1. Things will be much worse when missing data are considered (@sec-curse).\n2. Data exploration might mean iterating this code many times.\n3. **Percentage agreement** is simpler than some other measures/statistics.\n\nHow can we use matrix multiplication to speed things up? We'll have to use some tricks but it's not that hard.\n\n## Percentage agreement the fast way {#sec-vectorization}\n\nHere is a simple case with yes and no responses for 2 items coded as 1s and 0s respectively:\n\n| item 1 | item 2 | agreement | yes-yes | no-no |\n|:------:|:------:|:---------:|:-------:|:-----:|\n|    0   |    0   |     1     |    0    |   1   |\n|    0   |    1   |     0     |    0    |   0   |\n|    1   |    0   |     0     |    0    |   0   |\n|    1   |    1   |     1     |    1    |   0   |\n\nWhat we want is a column like *agreement* above whose sum (`2`) gives us the number of agreements. We then divide 2 by 4 to get 50-percent agreement.\n\nTreating **item 1** as a row vector and **item 2** as a column vector, we can perform matrix multiplication to get the sum of **yes-yes** (`1`). As an intermediate step in matrix multiplication, our 2 vectors are multiplied value-by-value giving us the yes-yes column above. Summing is the final step of matrix multiplication (`1`). \n\nAgreements can be either **yes-yes** or **no-no** so we still need to obtain that sum before we can measure number of agreements. That's easy. We simply flip the values in **item 1** and **item 2** from 0 to 1, and 1 to 0:\n\n$$\nnew = \\lvert old-1 \\rvert\n$$\n\nMatrix multiplication of these complementary vectors for **item 1** and **item 2** (not shown here) gives us our sum of **no-no**, whose intermediate step is the **no-no** column above.\n\nThe above example is for 2 items but the power of matrix multiplication is that we are multiplying **matrices** -- as many items as we want. And the result is a matrix of all pairwise comparisons. In other words, we can obtain a matrix of percentage agreement for all pairwise comparisons using a simple sequence of matrix-based operations, like this:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef pa_vect(X):\n    number_samples, number_variables = X.shape # (n x k)\n    yesYes = np.dot(X.transpose(), X)          # counts of yes-yes (k x k)\n    np.abs(X-1, out=X)                         # [0,1] -> [1,0]\n    noNo = np.dot(X.transpose(), X)            # counts of no-no   (k x k)\n    S = yesYes + noNo                          # counts of agreements (k x k)\n    A = S / number_samples                     # percentage agreements (k x k)   \n    return(A)\n```\n:::\n\n\nHow much faster is this compared to the loop-based computation?\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nnumber_samples = 1000\nnumber_variables = [10, 50, 100, 500, 1000]\nseconds = np.zeros((len(number_variables), 2))\nfor i, nvl in enumerate(number_variables):\n    X = np.random.choice([0., 1.], size=(number_samples, nvl), replace=True)\n    tic = perf_counter(); percent_agreement = pa_loop(X); toc = perf_counter()\n    seconds_loop = toc-tic\n    tic = perf_counter(); percent_agreement = pa_vect(X); toc = perf_counter()\n    seconds_vect = toc-tic\n    seconds[i, 0] = seconds_loop\n    seconds[i, 1] = seconds_vect\n    \ndf = pd.DataFrame(seconds, \n                  columns=['loop','matrix-based'],\n                  index=number_variables)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loop</th>\n      <th>matrix-based</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>0.000955</td>\n      <td>0.026229</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.015662</td>\n      <td>0.002517</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.060738</td>\n      <td>0.001671</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>1.158148</td>\n      <td>0.122394</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>4.396172</td>\n      <td>0.148437</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's examine the speed up graphically:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\ndf.plot(loglog=True, xlabel=\"Number of variables\", ylabel=\"Seconds\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=599 height=431}\n:::\n:::\n\n\nMatrix-based computation is definitely faster!\n\nHow much faster exactly? \n\nDividing the time taken by loop-based computation by the time taken by matrix-based computation gives us a speed-up. The larger the number the stronger the advantage for matrix-based computation:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nspeed_up = seconds[:,0] / seconds[:,1]\nplt.semilogx(number_variables, speed_up)\nplt.xlabel(\"Number of variables\")\nplt.ylabel(\"Speed advantage\")\nplt.title(\"Matrix-based operation beats loops by miles\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=585 height=451}\n:::\n:::\n\n\n## The curse of missing data {#sec-curse}\n\nNumpy is efficient because its data structures (matrices, vectors) have homogeneous elements. Everything in a numpy matrix for example is `int32` or whatever data type you set it to be (e.g., `float64`).\n\nIf you have an element that is undefined for some reason, like a missing value for example, then it is usually represented as `nan`. That stands for not-a-number and is common in programming languages. Note that `nan` is only defined for floating point data types.\n\nFor convenience, let's assign a variable to represent `np.nan` so we do not have to type `np.nan` all the time:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nM = np.nan\n```\n:::\n\n\nNow let's see why missing values in Numpy (`nan`) can be a problem for our **MM** solutions.\n\nIf we do any arithmetic that involves a `nan` we get a `nan` as a result:\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nprint(f\"10 + nan is {10 + M}\")\nprint(f\"23 * nan is {23 * M}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10 + nan is nan\n23 * nan is nan\n```\n:::\n:::\n\n\nSo if we do matrix multiplication, we will get `nan` for every pair that has at least one `nan`:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nx = np.array([1, 2, 3, 4]).reshape((4,1))\ny = np.array([1, 2, 3, M]).reshape((4,1))\nmm = np.dot(x.transpose(), y)\nprint(f\"matrix multiplication of {x.flatten()} and {y.flatten()} is {mm}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmatrix multiplication of [1 2 3 4] and [ 1.  2.  3. nan] is [[nan]].\n```\n:::\n:::\n\n\nNow let's see how we can deal with missing values in data sets.\n\nThere are simple cases which are uncommon, and there are complex cases which are common.\n\nHere is an extremely simple case:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nX = np.array([[0, 0, 0, 1],\n              [0, 0, 1, 1],\n              [1, 1, 0, 0],\n              [1, 1, 1, 0],\n              [M, M, M, M]])\ncol_labels = ['Item ' + str(i) for i in range(1,5)]\nrow_labels = ['Person ' + str(i) for i in range(1,6)]\nprint(pd.DataFrame(X, columns=col_labels, index=row_labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Item 1  Item 2  Item 3  Item 4\nPerson 1     0.0     0.0     0.0     1.0\nPerson 2     0.0     0.0     1.0     1.0\nPerson 3     1.0     1.0     0.0     0.0\nPerson 4     1.0     1.0     1.0     0.0\nPerson 5     NaN     NaN     NaN     NaN\n```\n:::\n:::\n\n\nHere we can perform [listwise deletion](https://en.wikipedia.org/wiki/Listwise_deletion), with no loss of information:\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nX[:-1,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([[0., 0., 0., 1.],\n       [0., 0., 1., 1.],\n       [1., 1., 0., 0.],\n       [1., 1., 1., 0.]])\n```\n:::\n:::\n\n\nBut the missing values in real data will often be in different records for different variables. Here is an extreme case:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nX = np.array([[0, 0, M, M],\n              [0, 0, M, M],\n              [1, 1, M, M],\n              [1, 1, M, M],\n              [0, M, 0, M],\n              [0, M, 1, M],\n              [1, M, 0, M],\n              [1, M, 1, M],\n              [0, M, M, 1],\n              [0, M, M, 1],\n              [1, M, M, 0],\n              [1, M, M, 0]])\n```\n:::\n\n\nIf we do listwise deletion we end up with no data.\n\nWhat we want is **pairwise deletion** instead of listwise deletion. We want to only exclude samples on a pairwise basis in order to retain the most information.\n\nThis is pretty straightforward with our loop-based code. We simply add list comprehensions inside the inner loop, which excludes samples whenever a value is missing for one of the pairs under consideration:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nimport math\n\ndef pa_loop_missing(X):\n    _, number_variables = X.shape\n    percent_agreement = np.zeros((number_variables, number_variables))\n    for item_a in range(number_variables):\n        for item_b in range(item_a+1, number_variables):\n                x = X[:, item_a]\n                y = X[:, item_b]\n                # remove observations where missing for either x or y\n                products = [a*b for a, b in zip(x,y)]\n                nx = [xi for xi, pi in zip(x, products) if not math.isnan(pi)]\n                ny = [yi for yi, pi in zip(y, products) if not math.isnan(pi)]\n                sumA = sum([va==vb for va, vb in zip(nx, ny)])\n                totA = len(nx)\n                percent_agreement[item_a, item_b] = [sumA / totA if totA > 0 else math.nan][0]\n    return(percent_agreement)\n  \npa_loop_missing(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([[0. , 1. , 0.5, 0. ],\n       [0. , 0. , nan, nan],\n       [0. , 0. , 0. , nan],\n       [0. , 0. , 0. , 0. ]])\n```\n:::\n:::\n\n\nThis works but as we will see in @sec-masked-arrays, this solution is very slow.\nThat is not a surprise since we now effectively have 3 nested loops.\n\nAnd if we try out matrix-based solution we end up no result except for the agreement of the first item with itself:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\npa_vect(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\narray([[ 1., nan, nan, nan],\n       [nan, nan, nan, nan],\n       [nan, nan, nan, nan],\n       [nan, nan, nan, nan]])\n```\n:::\n:::\n\n\nOf course. We already knew this. \n\nIn fact, it seems impossible to avoid for-loops if one wants to perform pairwise deletion.\n\nOr is it?\n\n\n## Masked arrays to the rescue {#sec-masked-arrays}\n\nAs it turns out, [numpy masked arrays](https://numpy.org/doc/stable/reference/maskedarray.html) effectively performs pairwise deletion when a matrix multiplication is performed!\n\nDespite being of very valuable, this feature of masked arrays is not something that appears to be commonly appreciated in discussions of what makes masked arrays useful.\n\nFor example, see the top-rated responses to this query on [Stack Overflow](https://stackoverflow.com/questions/55987642/why-are-numpy-masked-arrays-useful) on *Why are Numpy masked arrays useful?*. As with these responses, practical examples from research or data science are typically not given.\n\nSo let's make sure pairwise deletion is indeed performed using a matrix multiplication of masked arrays. I was not sure myself and had to check using numerous examples. Here is a really simple example I showed you before:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nX = np.array([[0, 0, M, M],\n              [0, 0, M, M],\n              [1, 1, M, M],\n              [1, 1, M, M],\n              [0, M, 0, M],\n              [0, M, 1, M],\n              [1, M, 0, M],\n              [1, M, 1, M],\n              [0, M, M, 1],\n              [0, M, M, 1],\n              [1, M, M, 0],\n              [1, M, M, 0]])\n\ncol_labels = ['Item ' + str(i) for i in range(1,X.shape[1]+1)]\nrow_labels = ['Person ' + str(i) for i in range(1,X.shape[0]+1)]\n\nprint(pd.DataFrame(X, columns=col_labels, index=row_labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Item 1  Item 2  Item 3  Item 4\nPerson 1      0.0     0.0     NaN     NaN\nPerson 2      0.0     0.0     NaN     NaN\nPerson 3      1.0     1.0     NaN     NaN\nPerson 4      1.0     1.0     NaN     NaN\nPerson 5      0.0     NaN     0.0     NaN\nPerson 6      0.0     NaN     1.0     NaN\nPerson 7      1.0     NaN     0.0     NaN\nPerson 8      1.0     NaN     1.0     NaN\nPerson 9      0.0     NaN     NaN     1.0\nPerson 10     0.0     NaN     NaN     1.0\nPerson 11     1.0     NaN     NaN     0.0\nPerson 12     1.0     NaN     NaN     0.0\n```\n:::\n:::\n\n\nClearly the sum of products between **item** 1 and itself should be `6`, between **item** 1 and 2 should be `2`, between **item** 1 and 3 should be `1`, and between **item** 1 and 4 should be `0`. You easily calculate the rest yourself.\n\nLet's see if matrix multiplication with masked arrays is consistent with pairwise deletion.\n\nThe first step is to import the masked array module, and convert `X` to a masked array such that `nan` values are identified as *masked*:\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nimport numpy.ma as ma\n\nR = ma.masked_invalid(X)\n```\n:::\n\n\nNow the rest is the easy. The masked array module contains almost all of the same functionality as numpy itself. So for example we can write `ma.dot()` instead of `np.dot()`:\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nyesYes = ma.dot(R.transpose(), R)\n\nprint(pd.DataFrame(yesYes, columns=col_labels, index=col_labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Item 1  Item 2  Item 3  Item 4\nItem 1     6.0     2.0     1.0     0.0\nItem 2     2.0     2.0     NaN     NaN\nItem 3     1.0     NaN     2.0     NaN\nItem 4     0.0     NaN     NaN     2.0\n```\n:::\n:::\n\n\nFantastic! Pairwise deletion without for loops!\n\nBut is this fast? Can we still get those juicy speed gains using matrix multiplication on masked arrays?\n\nLet's find out. First let's re-code our function for matrix-computation of percentage agreement to perform all operations on a masked-array version of the data:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndef pa_vect_missing(X):\n    R = ma.masked_invalid(X)                # (n x k)\n    yesYes = ma.dot(R.transpose(), R)       # counts of yes-yes (k x k)\n    F = ma.abs(R-1)                         # [0,1] -> [1,0]\n    noNo = ma.dot(F.transpose(), F)         # counts of no-no   (k x k)\n    S = yesYes + noNo                       # counts of agreements (k x k)\n    valid = np.ones_like(R)                 # valid responses (n x k)\n    valid[ma.getmaskarray(R)] = 0\n    N = np.dot(valid.transpose(), valid)    # valid count (k x k)\n    A = ma.multiply(S, N**-1)               # percentage agreement (k x k)\n    return(A)\n```\n:::\n\n\nAnd now let's compare our new loop- and matrix-based functions across a range of data sizes. Here I simulate binary responses with 20-percent missing data on average, randomly dispersed across sample and variable:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nresponses, probs = [0., 1., M], [.4, .4, .2]\nnumber_samples_large = 1000\nnumber_variables_large = [10, 50, 100, 500, 1000]\n\nseconds_missing = np.zeros((len(number_variables_large), 2))\nfor i, nvl in enumerate(number_variables_large):\n    print(str(i) + \"\\n\")\n    X = np.random.choice(responses, \n                        size=(number_samples_large, nvl), \n                        replace=True,\n                        p=probs)\n    tic = perf_counter(); percent_agreement = pa_loop_missing(X); toc = perf_counter()\n    seconds_loop = toc-tic\n    tic = perf_counter(); percent_agreement = pa_vect_missing(X); toc = perf_counter()\n    seconds_vect = toc-tic\n    seconds_missing[i, 0] = seconds_loop\n    seconds_missing[i, 1] = seconds_vect\n\ndf_missing = pd.DataFrame(seconds_missing, \n                          columns=['loop','vectorized'],\n                          index=number_variables_large)\n\ndf.plot(loglog=True, xlabel=\"Number of variables\", ylabel=\"Seconds\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n2\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n3\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n4\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-6.png){width=599 height=431}\n:::\n:::\n\n\nWow, is that an almost 1000-fold speed advantage for matrix-based computation?\n\nAgain let's show ratios of time taken to see:\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nspeed_up = seconds_missing[:,0] / seconds_missing[:,1]\nplt.semilogx(number_variables, speed_up)\nplt.xlabel(\"Number of variables\")\nplt.ylabel(\"Speed advantage\")\nplt.title(\"Matrix-based operation beats loops up to 1000 fold\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){width=602 height=451}\n:::\n:::\n\n\nYes, indeed. An almost 1000-fold speed advantage!\n\nObviously you'd want to run a speed test multiple times but I do not think minor changes in computer state or exact data would change all that much. Nonetheless, you may want to know about my set up. I ran this on an AMD Ryzen 7 5800H with 32GB ram.\n\nOh yeah, I also mentioned something about [**Cohen's kappa coefficient**](https://en.wikipedia.org/wiki/Cohen's_kappa). This is something you may want to consider using if you want to take into the degree to which agreements can happen by chance. Cohen's kappa is currently implemented in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) but only handles single pairs. If you wanted to measure kappa among a large number of variables using scikit-learn then you'd have to put their function into a nested loop.\n\nSo I wrote a matrix-based version of cohen's kappa that can handle missing values efficiently and I put it in this [GitHub repo](https://github.com/SourCherries/agreemat). It is basically like `pa_vect_missing()` but with some additional lines of code. So if you understood the discussion so far, then understanding my function for Cohen's kappa should be straightforward.\n\nI do not think that my solution for Cohen's kappa would be useful for [scikit-learn](https://scikit-learn.org/stable/index.html), as Cohen's kappa is primarily used there for model comparison with ground-truth labels. Unless you wanted to obtain a detailed picture of categorization errors among hundreds of thousands of models ... I used my code for simple data exploration.\n\n## Nasty details for those interested {#sec-details}\n\nIf you want to implement your own solution, you may want to know about Numpy optimization if you're not getting the gains you hoped for. \n\nI highly recommend the [blog post by Shih-Chin on this topic](http://shihchinw.github.io/2019/03/performance-tips-of-numpy-ndarray.html). It is a well written and overall excellent resource!\n\nHere I point out one little detail you might have missed in the above code:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nXa = np.random.choice([0., 1.], size=(1000, 1000), replace=True)\nXb = np.random.choice([0,  1],  size=(1000, 1000), replace=True)\nprint(f\"Xa has data type {Xa.dtype}.\")\nprint(f\"Xb has data type {Xb.dtype}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nXa has data type float64.\nXb has data type int64.\n```\n:::\n:::\n\n\nYou might remember that the presence of missing values will automatically cast data type to `float`. So all the data in @sec-curse were `float64`.\n\nWhen I wrote about computation without missing values (@sec-vectorization), I was careful to keep this consistent and use floating value: like `Xa` in the above code and not like `Xb`! Why be so careful?\n\nAs it turns out, this can have an effect on computational speed.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ntic = perf_counter(); np.dot(Xa.transpose(), Xa); toc = perf_counter()\nsecs_order_1 = toc - tic\n\ntic = perf_counter(); np.dot(Xa, Xa.transpose()); toc = perf_counter()\nsecs_order_2 = toc - tic\n\nprint(f\"np.dot(Xa.transpose(), Xa) took {secs_order_1} seconds.\")\nprint(f\"np.dot(Xa, Xa.transpose()) took {secs_order_2} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnp.dot(Xa.transpose(), Xa) took 0.1135176779998801 seconds.\nnp.dot(Xa, Xa.transpose()) took 0.05753454500063526 seconds.\n```\n:::\n:::\n\n\nIf your data type is float, as it must be when we have missing data, then you will want to matrix multiply the transpose of X with X rather than the other way around. The former is faster.\n\nObviously, the transpose of X came first in our calculations because we set the columns of our data to be the variables. That way we get a pairwise comparisons of variables instead of samples. But clearly, we could have set the rows of our data to be the variables, in which case X transpose should come second.\n\nNow what happens if our data type is `int64` instead of `float64`?\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ntic = perf_counter(); np.dot(Xb.transpose(), Xb); toc = perf_counter()\nsecs_order_1 = toc - tic\n\ntic = perf_counter(); np.dot(Xb, Xb.transpose()); toc = perf_counter()\nsecs_order_2 = toc - tic\n\nprint(f\"np.dot(Xb.transpose(), Xb) took {secs_order_1} seconds.\")\nprint(f\"np.dot(Xb, Xb.transpose()) took {secs_order_2} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnp.dot(Xb.transpose(), Xb) took 2.4103485059995364 seconds.\nnp.dot(Xb, Xb.transpose()) took 1.174356636000084 seconds.\n```\n:::\n:::\n\n\nNow things are much slower overall and we get the opposite pattern -- putting X transpose to the right side of our matrix multiplication is faster!\n\nWhy is this? That is not what this blog post is about. Take a deep dive into [the blog post by Shih-Chin](http://shihchinw.github.io/2019/03/performance-tips-of-numpy-ndarray.html).\n\nFinally, this discussion ignores statistical issues in inference when we are performing pairwise deletion. If you are simply exploring your data or trying out different machine learning methods, then that's not such an issue. But keep in mind that the effect of pairwise deletion on inference are likely to be tricky.\n\n## Conclusion {#sec-takeaway}\n\nIf you need to compute a measure/statistic for all pairwise comparisons among a large set of variables, then you can probably speed things up a lot with matrix multiplications. It may take some creativity, but hopefully this blog post can provide some inspiration.\n\n**Matrix multiplication using masked arrays does pairwise deletion and that's very useful to know!**\n\nSo if you also have missing data, then you can use matrix multiplications to obtain even more gains in efficiency! Simply use Numpy masked arrays.\n\nIf you do decide to try out a similar solution for your measure/statistic, pay heed to @sec-details. And watch out for the Numpy acceleration package [Jax](https://jax.readthedocs.io/en/latest/index.html). While it currently does not support **masked arrays**, perhaps it or some external package using Jax will do so in the future. That could be very useful.\n\nThe solution I present here for **percentage agreement** and **Cohen's kappa** is not currently implemented in any major Python package.\n\nFinally, watch out for my next post on how missing data are dealt with by current solutions that measure the **covariance matrix**, and why that matters.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}